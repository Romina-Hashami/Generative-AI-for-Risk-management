# -*- coding: utf-8 -*-
"""LLM_prediction.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1ntNuBRloUG9x04vERiqWfc-mCDsiN3N3
"""

import pandas as pd
import numpy as np

import numpy as np
import pandas as pd

# --------- Load 5-min data ----------
df = pd.read_csv(
    "/content/cb-5m.csv",
    delimiter=";",
    names=["Date", "Time", "Open", "High", "Low", "Close", "Volume"]
)

# Parse datetime
df["Datetime"] = pd.to_datetime(
    df["Date"].astype(str) + " " + df["Time"].astype(str),
    format="%d/%m/%Y %H:%M:%S",
    errors="raise"
)

# Sort (important!)
df = df.sort_values("Datetime").reset_index(drop=True)

# Keep a daily date key (normalized timestamp, not python date)
df["Date_d"] = df["Datetime"].dt.normalize()

# --------- Intraday log returns ----------
# NOTE: scaling factor: keep yours but name it clearly
SCALE = 40.0
df["r"] = np.log(df["Close"].astype(float) / df["Close"].astype(float).shift(1))
df["r"] = df["r"] * SCALE

# If there are day breaks, the first return of a day uses last bar of prev day.
# That's fine for RV; if you want "within-day only", set those to NaN:
first_of_day = df["Date_d"].ne(df["Date_d"].shift(1))
df.loc[first_of_day, "r"] = np.nan

# --------- Daily RV ----------
df["r2"] = df["r"] ** 2
rv = df.groupby("Date_d")["r2"].sum(min_count=1).reset_index(name="RV")

# --------- Daily bipower variation (simple) ----------
# Your structure: |r_{t-1}| * |r_t|
df["absr"] = df["r"].abs()
df["bvp_term"] = df["absr"].shift(1) * df["absr"]
df.loc[first_of_day, "bvp_term"] = np.nan  # avoid cross-day product
bvp = df.groupby("Date_d")["bvp_term"].sum(min_count=1).reset_index(name="BVP")

# --------- Signed RV ----------
df["rv_neg_term"] = np.where(df["r"] < 0, df["r2"], 0.0)
df["rv_pos_term"] = np.where(df["r"] > 0, df["r2"], 0.0)
rv_neg = df.groupby("Date_d")["rv_neg_term"].sum(min_count=1).reset_index(name="RV_neg")
rv_pos = df.groupby("Date_d")["rv_pos_term"].sum(min_count=1).reset_index(name="RV_pos")

# --------- Realized quarticity ----------
df["rq_term"] = df["r"] ** 4
rq = df.groupby("Date_d")["rq_term"].sum(min_count=1).reset_index(name="RQ")

# --------- Daily close-to-close return ----------
# Use last close of each day
daily_close = df.groupby("Date_d")["Close"].last().astype(float).reset_index(name="Close_d")
daily_close["ret_d"] = np.log(daily_close["Close_d"] / daily_close["Close_d"].shift(1))
daily_close["ret_abs_d"] = daily_close["ret_d"].abs()
daily_close["ret2_d"] = daily_close["ret_d"] ** 2

# --------- Merge all daily measures ----------
result = rv.merge(bvp, on="Date_d", how="inner") \
           .merge(rv_neg, on="Date_d", how="inner") \
           .merge(rv_pos, on="Date_d", how="inner") \
           .merge(rq, on="Date_d", how="inner") \
           .merge(daily_close[["Date_d", "ret_d", "ret_abs_d", "ret2_d"]], on="Date_d", how="left")

# Final column name to match your pipeline
result = result.rename(columns={"Date_d": "Date"})

# Ensure Date is datetime64[ns]
result["Date"] = pd.to_datetime(result["Date"]).dt.normalize()

print(result.head())
print("Rows:", len(result), "Date range:", result["Date"].min(), "→", result["Date"].max())

df = pd.read_csv('/content/all_news_data.csv')
df = df.drop('Unnamed: 0', axis = 1)

df

df['date_column'] = pd.to_datetime(df['date_column'])
result['Date'] = pd.to_datetime(result['Date'])

# Merge them
merged = pd.merge(df, result, left_on='date_column', right_on='Date', how='inner')
merged

# ============================================================
# TOTAL CODE (HORIZON = 1) — Leak-free Rolling
# HAR vs LLM vs Hybrid + Paper Tables (2,3,7,9)
#
# CRITICAL FIX:
#   LLM output is forced at TOKEN LEVEL to be EXACTLY: "<d> <s>"
#   where d ∈ {0..6} and s ∈ {0..9}
#   => parsing succeeds ~100% (fallback ~0), no more "single digit" failures.
#
# Pipeline (unchanged conceptually):
#   1) HAR forecast (logRV HAR)
#   2) LLM gives shock ∈ [-3,3] (continuous) from news
#   3) Calibrate m = exp(a + g*shock) on past-only window (Huber robust)
#   4) LLM forecast = HAR * m (clipped)
#   5) Hybrid = geometric mix with rolling alpha_t chosen on past-only window
#
# OUTPUTS:
#   /content/results/forecasts_H1.csv
#   /content/results/metrics_H1.csv
#   /content/results/dm_H1.csv
#   /content/results/main_results_H1.tex
#   /content/results/paper_tables/Table2_*.csv/.tex
#   /content/results/paper_tables/Table3_*.csv/.tex
#   /content/results/paper_tables/Table7_*.csv/.tex
#   /content/results/paper_tables/Table9_*.csv/.tex
# ============================================================

import os, re, math, warnings
from dataclasses import dataclass
from collections import deque
from typing import List, Tuple, Optional

import numpy as np
import pandas as pd
from tqdm.auto import tqdm
warnings.filterwarnings("ignore")

# ----------------------------
# CONFIG
# ----------------------------
@dataclass
class Cfg:
    col_text: str = "text"
    col_rv: str = "RV"
    col_date_pref: str = "Date"
    col_date_alt: str = "date_column"

    h: int = 1

    train_win: int = 504
    news_days: int = 60
    topk_context: int = 8

    # your best earlier hybrid used k=3; set to 1 for stronger signal (slower)
    llm_every_k_days: int = 3
    llm_retries: int = 1

    cal_window: int = 252
    alpha_window: int = 252

    m_lo: float = 0.60
    m_hi: float = 1.60
    clip_a: float = 0.25
    clip_g: float = 0.30

    alpha_grid: Tuple[float, ...] = tuple(np.round(np.linspace(0, 1, 21), 2))

    # generation
    max_input_tokens: int = 220
    max_new_tokens: int = 4  # enough for: digit, space, digit, eos

    out_dir: str = "/content/results"
    tables_dir: str = "/content/results/paper_tables"

    # subsamples (draft)
    pre_start: str = "2014-01-01"
    pre_end: str   = "2019-12-31"
    post_start: str = "2020-01-01"
    post_end: str   = "2023-12-31"

    # capital proxy
    portfolio_usd: float = 100_000_000.0
    z_99: float = 2.326

cfg = Cfg()
os.makedirs(cfg.out_dir, exist_ok=True)
os.makedirs(cfg.tables_dir, exist_ok=True)

# ----------------------------
# REQUIRE merged dataframe
# ----------------------------
try:
    merged  # noqa
except NameError:
    raise RuntimeError("You must have a DataFrame named `merged` loaded with columns including text/date/RV.")

# ----------------------------
# METRICS + DM
# ----------------------------
def qlike_mean(y, f):
    y = np.asarray(y, float)
    f = np.clip(np.asarray(f, float), 1e-12, None)
    return float(np.mean(np.log(f) + (y / f)))

def mse_log_mean(y, f):
    y = np.asarray(y, float)
    f = np.asarray(f, float)
    return float(np.mean((np.log(np.clip(y,1e-12,None)) - np.log(np.clip(f,1e-12,None)))**2))

def qlike_loss(y, f):
    y = np.asarray(y, float)
    f = np.clip(np.asarray(f, float), 1e-12, None)
    return np.log(f) + y / f

def dm_test(loss_a, loss_b):
    d = np.asarray(loss_a, float) - np.asarray(loss_b, float)
    d = d[np.isfinite(d)]
    T = len(d)
    if T < 30:
        return {"DM_stat": np.nan, "p_value": np.nan, "N": int(T)}
    dbar = float(np.mean(d))
    var = float(np.var(d, ddof=1)) / T
    dm = dbar / math.sqrt(var + 1e-12)
    from math import erf, sqrt
    cdf = 0.5 * (1.0 + erf(abs(dm)/sqrt(2.0)))
    p = 2.0 * (1.0 - cdf)
    return {"DM_stat": float(dm), "p_value": float(p), "N": int(T)}

# ----------------------------
# TEXT HELPERS
# ----------------------------
def clean_text(s: str) -> str:
    return re.sub(r"\s+", " ", str(s)).strip()

def select_bullets_recent_unique(pool_bullets, k: int):
    bullets = [clean_text(b) for b in pool_bullets if isinstance(b, str)]
    bullets = [b for b in bullets if b]
    seen, out = set(), []
    for b in reversed(bullets):
        if b not in seen:
            seen.add(b)
            out.append(b)
        if len(out) >= k:
            break
    return list(reversed(out))

# ----------------------------
# BUILD DAILY PANEL
# ----------------------------
df = merged.copy()
date_col = cfg.col_date_pref if cfg.col_date_pref in df.columns else cfg.col_date_alt

for c in [date_col, cfg.col_text, cfg.col_rv]:
    if c not in df.columns:
        raise ValueError(f"Missing '{c}' in merged. Found: {df.columns.tolist()}")

df[date_col] = pd.to_datetime(df[date_col], errors="coerce").dt.normalize()
df[cfg.col_rv] = pd.to_numeric(df[cfg.col_rv], errors="coerce")
df = df.dropna(subset=[date_col, cfg.col_rv]).copy()
df = df[df[cfg.col_rv] > 0].copy()

daily_rv = (
    df[[date_col, cfg.col_rv]]
    .drop_duplicates(subset=[date_col], keep="last")
    .sort_values(date_col)
    .rename(columns={date_col: "date", cfg.col_rv: "RV"})
    .reset_index(drop=True)
)

daily_news = (
    df[[date_col, cfg.col_text]]
    .dropna(subset=[cfg.col_text])
    .groupby(date_col)[cfg.col_text].apply(list)
    .reset_index(name="news_list")
    .rename(columns={date_col: "date"})
)

daily = daily_rv.merge(daily_news, on="date", how="left")
daily["news_list"] = daily["news_list"].apply(lambda x: x if isinstance(x, list) else [])
daily = daily.sort_values("date").reset_index(drop=True)

daily["logRV"] = np.log(daily["RV"].astype(float))
daily["D"] = daily["logRV"].shift(1)
daily["W"] = daily["logRV"].shift(1).rolling(5, min_periods=5).mean()
daily["M"] = daily["logRV"].shift(1).rolling(22, min_periods=22).mean()

daily["RV_next"] = daily["RV"].shift(-cfg.h)
daily["logRV_next"] = np.log(daily["RV_next"].clip(1e-12))
daily = daily.dropna(subset=["D","W","M","RV_next","logRV_next"]).reset_index(drop=True)

print("Daily rows:", len(daily), "| range:", daily["date"].min().date(), "→", daily["date"].max().date())

# ----------------------------
# LOAD LLM + STRICT FORMAT DECODING
# ----------------------------
import torch
from transformers import AutoTokenizer, AutoModelForCausalLM, LogitsProcessor, LogitsProcessorList

MODEL_NAME = "Qwen/Qwen2.5-1.5B-Instruct"

if not torch.cuda.is_available():
    raise RuntimeError("CUDA GPU required.")

tok = AutoTokenizer.from_pretrained(MODEL_NAME, use_fast=True)
if tok.pad_token_id is None:
    tok.pad_token = tok.eos_token

model = AutoModelForCausalLM.from_pretrained(
    MODEL_NAME,
    device_map={"": 0},
    torch_dtype=torch.float16,
    low_cpu_mem_usage=True
).eval()
torch.set_grad_enabled(False)

def _single_token_id(tokenizer, s: str):
    ids = tokenizer.encode(s, add_special_tokens=False)
    return ids[0] if len(ids) == 1 else None

SPACE_ID = _single_token_id(tok, " ")
EOS_ID = tok.eos_token_id
DIGIT_IDS = {str(i): _single_token_id(tok, str(i)) for i in range(10)}

if SPACE_ID is None or any(v is None for v in DIGIT_IDS.values()):
    raise RuntimeError(
        "Tokenizer does not map digits/space to single tokens. "
        "Use a different model/tokenizer or remove strict decoding."
    )

DIGITS_0_6 = [DIGIT_IDS[str(i)] for i in range(7)]
DIGITS_0_9 = [DIGIT_IDS[str(i)] for i in range(10)]

class ForceTwoDigitFormat(LogitsProcessor):
    """
    Forces generated tokens (after prompt) to be:
      step 0: digit 0..6
      step 1: space
      step 2: digit 0..9
      step >=3: eos
    """
    def __init__(self, prompt_len: int, d06: List[int], d09: List[int], space_id: int, eos_id: int):
        self.prompt_len = prompt_len
        self.d06 = d06
        self.d09 = d09
        self.space = space_id
        self.eos = eos_id

    def __call__(self, input_ids, scores):
        step = input_ids.shape[1] - self.prompt_len
        if step == 0:
            allowed = self.d06
        elif step == 1:
            allowed = [self.space]
        elif step == 2:
            allowed = self.d09
        else:
            allowed = [self.eos]

        mask = torch.full_like(scores, float("-inf"))
        mask[:, torch.tensor(allowed, device=scores.device)] = 0.0
        return scores + mask

def llm_generate_two_digits(prompt: str) -> str:
    inputs = tok(prompt, return_tensors="pt", truncation=True, max_length=cfg.max_input_tokens).to(model.device)
    prompt_len = int(inputs["input_ids"].shape[1])

    logits_proc = LogitsProcessorList([
        ForceTwoDigitFormat(prompt_len, DIGITS_0_6, DIGITS_0_9, SPACE_ID, EOS_ID)
    ])

    with torch.no_grad():
        out = model.generate(
            **inputs,
            max_new_tokens=cfg.max_new_tokens,
            do_sample=False,
            pad_token_id=EOS_ID,
            eos_token_id=EOS_ID,
            logits_processor=logits_proc,
        )

    gen = out[0][prompt_len:]
    txt = tok.decode(gen, skip_special_tokens=True)
    return (txt or "").strip()

# ----------------------------
# SHOCK: "d s" => continuous shock in [-3,3]
# ----------------------------
DIR_CLASS_TO_DIR = {0:-3, 1:-2, 2:-1, 3:0, 4:1, 5:2, 6:3}

def llm_shock_continuous(bullets: List[str]) -> Tuple[float, bool, str]:
    bullets_txt = "\n".join([f"- {str(b)[:160]}" for b in bullets]) if bullets else "- (no notable news)"

    SYSTEM = "Return ONLY two digits: d s (one space)."
    USER = f"""
NEWS:
{bullets_txt}

Return ONLY: d s

d is direction:
0=-3, 1=-2, 2=-1, 3=0, 4=+1, 5=+2, 6=+3
s is strength:
0..9 meaning 0.0..1.0

Examples:
No material info -> 3 0
High uncertainty / conflict -> 6 8
Mild uncertainty -> 4 4
Calming / de-escalation -> 0 6

Now output:
""".strip()

    msgs = [{"role":"system","content":SYSTEM},{"role":"user","content":USER}]
    prompt = tok.apply_chat_template(msgs, tokenize=False, add_generation_prompt=True)

    last = ""
    for _ in range(cfg.llm_retries + 1):
        last = llm_generate_two_digits(prompt)
        digs = re.findall(r"\b(\d)\b", last)
        if len(digs) >= 2:
            d = int(digs[0]); s = int(digs[1])
            if 0 <= d <= 6 and 0 <= s <= 9:
                direction = DIR_CLASS_TO_DIR[d]
                strength = s / 9.0
                return float(direction) * float(strength), True, last

    # should not happen with strict decoding
    return 0.0, False, last

# ----------------------------
# HAR + ROBUST CALIBRATION (Huber)
# ----------------------------
def fit_har(train_df: pd.DataFrame):
    X = np.column_stack([np.ones(len(train_df)), train_df["D"].values, train_df["W"].values, train_df["M"].values])
    y = train_df["logRV_next"].values
    beta, *_ = np.linalg.lstsq(X, y, rcond=None)
    return beta

def har_forecast(beta, row: pd.Series) -> float:
    x = np.array([1.0, row["D"], row["W"], row["M"]], float)
    return float(np.exp(x @ beta))

def estimate_a_g_huber(pairs: List[Tuple[float, float]]):
    if len(pairs) < 80:
        return 0.0, 0.05
    s = np.array([p[0] for p in pairs], float)
    r = np.array([p[1] for p in pairs], float)
    X = np.column_stack([np.ones(len(s)), s])

    beta, *_ = np.linalg.lstsq(X, r, rcond=None)
    k = 1.345
    for _ in range(12):
        resid = r - (X @ beta)
        scale = np.median(np.abs(resid)) / 0.6745 + 1e-8
        u = resid / scale
        w = np.ones_like(u)
        m = np.abs(u) > k
        w[m] = k / np.abs(u[m])
        W = w[:, None]
        beta_new, *_ = np.linalg.lstsq(X * W, r * w, rcond=None)
        if np.max(np.abs(beta_new - beta)) < 1e-6:
            beta = beta_new
            break
        beta = beta_new

    a = float(np.clip(beta[0], -cfg.clip_a, cfg.clip_a))
    g = float(np.clip(beta[1], -cfg.clip_g, cfg.clip_g))
    return a, g

def choose_alpha(y_hist, har_hist, llm_hist):
    if len(y_hist) < 80:
        return 0.5
    best_a, best = 0.5, float("inf")
    for a in cfg.alpha_grid:
        hyb = np.exp(a*np.log(np.clip(har_hist,1e-12,None)) + (1-a)*np.log(np.clip(llm_hist,1e-12,None)))
        loss = qlike_mean(y_hist, hyb)
        if loss < best:
            best = loss
            best_a = a
    return float(best_a)

# ----------------------------
# MAIN ROLLING LOOP
# ----------------------------
pool = deque()
def add_day_news(i: int):
    d = daily.loc[i, "date"]
    for b in daily.loc[i, "news_list"]:
        b = clean_text(b)
        if b:
            pool.append((d, b))

def evict_old(cur_date):
    cutoff = cur_date - pd.Timedelta(days=cfg.news_days)
    while pool and pool[0][0] < cutoff:
        pool.popleft()

warm = min(len(daily), cfg.train_win + 10)
for i in range(warm):
    add_day_news(i)

ag_pairs = deque(maxlen=cfg.cal_window)
y_hist = deque(maxlen=cfg.alpha_window)
har_hist = deque(maxlen=cfg.alpha_window)
llm_hist = deque(maxlen=cfg.alpha_window)

last_valid_shock = 0.0
LLM_CALLS = 0
FAIL_CALLS = 0
records = []

end_t = len(daily) - 1

for t in tqdm(range(cfg.train_win, end_t), desc=f"H1 MAIN (strict 'd s', k={cfg.llm_every_k_days})"):
    info_date = daily.loc[t, "date"]
    evict_old(info_date)
    bullets = select_bullets_recent_unique([b for (_, b) in pool], k=cfg.topk_context)

    tr = daily.iloc[t-cfg.train_win:t].copy()
    beta = fit_har(tr)
    har_pred = har_forecast(beta, daily.loc[t])

    call_today = ((t - cfg.train_win) % cfg.llm_every_k_days == 0) or (t == cfg.train_win)
    raw_out = ""
    if call_today:
        shock, ok, raw_out = llm_shock_continuous(bullets)
        LLM_CALLS += 1
        if ok:
            last_valid_shock = shock
        else:
            FAIL_CALLS += 1
            shock = last_valid_shock
    else:
        shock = last_valid_shock
        ok = True

    a, g = estimate_a_g_huber(list(ag_pairs))
    m = float(np.exp(a + g * float(shock)))
    m = float(np.clip(m, cfg.m_lo, cfg.m_hi))
    llm_pred = float(har_pred * m)

    alpha_t = choose_alpha(np.array(y_hist), np.array(har_hist), np.array(llm_hist))
    hybrid = float(np.exp(alpha_t*np.log(max(har_pred,1e-12)) + (1-alpha_t)*np.log(max(llm_pred,1e-12))))

    y_true = float(daily.loc[t, "RV_next"])
    log_ratio = float(np.log(max(y_true,1e-12)) - np.log(max(har_pred,1e-12)))

    ag_pairs.append((float(shock), log_ratio))
    y_hist.append(y_true); har_hist.append(har_pred); llm_hist.append(llm_pred)

    records.append({
        "h": 1,
        "date_t": info_date,
        "RV_next": y_true,
        "HAR": har_pred,
        "shock": float(shock),
        "a": float(a),
        "g": float(g),
        "m": float(m),
        "LLM_pred": llm_pred,
        "alpha_t": float(alpha_t),
        "Hybrid": hybrid,
        "LLM_called_today": int(call_today),
        "LLM_call_failed": int(call_today and (not ok)),
        "LLM_raw_out": raw_out[:10],
        "log_ratio": float(log_ratio),
    })

    if t + 1 < len(daily):
        add_day_news(t + 1)

fc = pd.DataFrame(records)
fc["date_t"] = pd.to_datetime(fc["date_t"]).dt.normalize()

# ----------------------------
# SAVE OUTPUTS + METRICS + DM
# ----------------------------
PATH_FC  = os.path.join(cfg.out_dir, "forecasts_H1.csv")
PATH_MET = os.path.join(cfg.out_dir, "metrics_H1.csv")
PATH_DM  = os.path.join(cfg.out_dir, "dm_H1.csv")
PATH_TEX = os.path.join(cfg.out_dir, "main_results_H1.tex")

fc.to_csv(PATH_FC, index=False)

mask = np.isfinite(fc["RV_next"]) & (fc["RV_next"] > 0)
for c in ["HAR","LLM_pred","Hybrid"]:
    mask &= np.isfinite(fc[c]) & (fc[c] > 0)

y   = fc.loc[mask, "RV_next"].values
har = fc.loc[mask, "HAR"].values
llm = fc.loc[mask, "LLM_pred"].values
hyb = fc.loc[mask, "Hybrid"].values

fallback_rate = float(fc.loc[fc["LLM_called_today"]==1, "LLM_call_failed"].mean()) if fc["LLM_called_today"].sum() else 0.0

met = pd.DataFrame([
    {"model":"HAR",    "N":int(mask.sum()), "QLIKE": qlike_mean(y, har), "MSElog": mse_log_mean(y, har)},
    {"model":"LLM",    "N":int(mask.sum()), "QLIKE": qlike_mean(y, llm), "MSElog": mse_log_mean(y, llm)},
    {"model":"Hybrid", "N":int(mask.sum()), "QLIKE": qlike_mean(y, hyb), "MSElog": mse_log_mean(y, hyb)},
])
met["h"] = 1
met["model_name"] = MODEL_NAME
met["llm_every_k_days"] = cfg.llm_every_k_days
met["news_days"] = cfg.news_days
met["topk_context"] = cfg.topk_context
met["LLM_calls"] = int(fc["LLM_called_today"].sum())
met["fallback_rate_among_calls"] = fallback_rate
met["cal_window"] = cfg.cal_window
met["m_bounds"] = f"[{cfg.m_lo},{cfg.m_hi}]"
met["alpha_window"] = cfg.alpha_window
met.to_csv(PATH_MET, index=False)

dm_df = pd.DataFrame([
    {"comparison":"HAR vs LLM",    **dm_test(qlike_loss(y, har), qlike_loss(y, llm))},
    {"comparison":"HAR vs Hybrid", **dm_test(qlike_loss(y, har), qlike_loss(y, hyb))},
    {"comparison":"LLM vs Hybrid", **dm_test(qlike_loss(y, llm), qlike_loss(y, hyb))},
])
dm_df.to_csv(PATH_DM, index=False)

with open(PATH_TEX, "w", encoding="utf-8") as f:
    f.write(met[["model","N","QLIKE","MSElog","fallback_rate_among_calls","LLM_calls"]].to_latex(index=False, float_format="%.4f"))

print("\n==================== MAIN RESULTS SAVED (H1) ====================")
print("Forecasts:", PATH_FC)
print("Metrics  :", PATH_MET)
print("DM tests :", PATH_DM)
print("LaTeX    :", PATH_TEX)

print("\n===== METRICS (H1) =====")
print(met)
print("\n===== DM TESTS (QLIKE loss) =====")
print(dm_df)

print("\nAUDIT:")
print("MODEL:", MODEL_NAME)
print("LLM_CALLS:", int(fc["LLM_called_today"].sum()))
print("FALLBACKS among calls:", int(fc.loc[fc["LLM_called_today"]==1, "LLM_call_failed"].sum()))
print("Fallback rate among calls:", round(100*fallback_rate, 3), "%")
print("Shock summary: mean", round(float(fc["shock"].mean()), 4),
      "| pct shock==0:", round(100*float((np.abs(fc["shock"])<1e-9).mean()), 2), "%")

# ============================================================
# TABLES FOR YOUR DRAFT (H=1): Table 2, 3, 7, 9
# ============================================================
os.makedirs(cfg.tables_dir, exist_ok=True)

# Table 2
table2 = met[["model","N","QLIKE","MSElog"]].copy()
t2_csv = os.path.join(cfg.tables_dir, "Table2_forecast_accuracy_H1.csv")
t2_tex = os.path.join(cfg.tables_dir, "Table2_forecast_accuracy_H1.tex")
table2.to_csv(t2_csv, index=False)
with open(t2_tex, "w", encoding="utf-8") as f:
    f.write(table2.to_latex(index=False, float_format="%.4f"))

# Table 3
table3 = dm_df.copy()
t3_csv = os.path.join(cfg.tables_dir, "Table3_DM_tests_H1.csv")
t3_tex = os.path.join(cfg.tables_dir, "Table3_DM_tests_H1.tex")
table3.to_csv(t3_csv, index=False)
with open(t3_tex, "w", encoding="utf-8") as f:
    f.write(table3.to_latex(index=False, float_format="%.4f"))

# Table 7
def eval_slice(df_slice: pd.DataFrame):
    ys = df_slice["RV_next"].values
    hs = df_slice["HAR"].values
    bs = df_slice["Hybrid"].values
    har_q = qlike_mean(ys, hs)
    hyb_q = qlike_mean(ys, bs)
    return {
        "N": int(len(df_slice)),
        "HAR QLIKE": har_q,
        "Hybrid QLIKE": hyb_q,
        "Improvement (%)": 100.0 * (har_q - hyb_q) / har_q
    }

pre = fc[(fc["date_t"] >= cfg.pre_start) & (fc["date_t"] <= cfg.pre_end)].copy()
post = fc[(fc["date_t"] >= cfg.post_start) & (fc["date_t"] <= cfg.post_end)].copy()

rows = []
rows.append({"Period": f"Pre-COVID ({cfg.pre_start[:4]}–{cfg.pre_end[:4]})", **eval_slice(pre)} if len(pre) >= 50
            else {"Period": f"Pre-COVID ({cfg.pre_start[:4]}–{cfg.pre_end[:4]})", "N": int(len(pre))})
rows.append({"Period": f"Post-COVID ({cfg.post_start[:4]}–{cfg.post_end[:4]})", **eval_slice(post)} if len(post) >= 50
            else {"Period": f"Post-COVID ({cfg.post_start[:4]}–{cfg.post_end[:4]})", "N": int(len(post))})

table7 = pd.DataFrame(rows)
t7_csv = os.path.join(cfg.tables_dir, "Table7_pre_post_covid_H1.csv")
t7_tex = os.path.join(cfg.tables_dir, "Table7_pre_post_covid_H1.tex")
table7.to_csv(t7_csv, index=False)
with open(t7_tex, "w", encoding="utf-8") as f:
    f.write(table7.to_latex(index=False, float_format="%.4f"))

# Table 9
mask_cap = (fc["HAR"] > 0) & (fc["Hybrid"] > 0)
har_cap = fc.loc[mask_cap, "HAR"].values
hyb_cap = fc.loc[mask_cap, "Hybrid"].values

VaR_har = cfg.portfolio_usd * cfg.z_99 * np.sqrt(har_cap)
VaR_hyb = cfg.portfolio_usd * cfg.z_99 * np.sqrt(hyb_cap)

table9 = pd.DataFrame([{
    "Portfolio (USD)": cfg.portfolio_usd,
    "z(99%)": cfg.z_99,
    "Avg Capital (HAR)": float(np.mean(VaR_har)),
    "Avg Capital (Hybrid)": float(np.mean(VaR_hyb)),
    "Savings (HAR - Hybrid)": float(np.mean(VaR_har) - np.mean(VaR_hyb)),
}])
t9_csv = os.path.join(cfg.tables_dir, "Table9_capital_proxy_H1.csv")
t9_tex = os.path.join(cfg.tables_dir, "Table9_capital_proxy_H1.tex")
table9.to_csv(t9_csv, index=False)
with open(t9_tex, "w", encoding="utf-8") as f:
    f.write(table9.to_latex(index=False, float_format="%.2f"))

print("\n==================== TABLES SAVED (H1) ====================")
print("Tables dir:", cfg.tables_dir)
print("Table 2:", t2_csv, "and", t2_tex)
print("Table 3:", t3_csv, "and", t3_tex)
print("Table 7:", t7_csv, "and", t7_tex)
print("Table 9:", t9_csv, "and", t9_tex)

import numpy as np
import pandas as pd

fc_path = "/content/results/forecasts_H1.csv"
fc = pd.read_csv(fc_path)
fc["date_t"] = pd.to_datetime(fc["date_t"]).dt.normalize()

# Safety
for c in ["RV_next","HAR","LLM_pred","Hybrid","shock"]:
    if c not in fc.columns:
        raise ValueError(f"Missing column {c} in forecasts_H1.csv")

def qlike_mean(y, f):
    y = np.asarray(y, float)
    f = np.clip(np.asarray(f, float), 1e-12, None)
    return float(np.mean(np.log(f) + y/f))

# Baselines
mask = (fc["RV_next"] > 0) & (fc["HAR"] > 0) & (fc["LLM_pred"] > 0)
y = fc.loc[mask, "RV_next"].values
har = fc.loc[mask, "HAR"].values
llm = fc.loc[mask, "LLM_pred"].values

print("=== BASE PERFORMANCE (from your run) ===")
print("HAR QLIKE:", qlike_mean(y, har))
print("LLM QLIKE:", qlike_mean(y, llm))

# --- Rebuild LLM_pred using existing HAR and shocks with same a,g? ---
# We can't fully reconstruct your dynamic a,g without rerunning.
# But we CAN run leakage tests by substituting shocks while keeping HAR baseline
# using a simple monotone mapping m = exp(k * shock) calibrated on the same sample.
# This is enough to detect timing leakage.

# Fit k to best explain log(y/har) ~ k*shock
mask2 = mask & np.isfinite(fc["shock"])
shock = fc.loc[mask2, "shock"].values
log_ratio = np.log(fc.loc[mask2, "RV_next"].values) - np.log(fc.loc[mask2, "HAR"].values)

# OLS k (simple for diagnostics)
k = float(np.linalg.lstsq(shock.reshape(-1,1), log_ratio, rcond=None)[0][0])
print("\nDiagnostic k (log-multiplier per unit shock):", round(k, 4))

def llm_from_shock(shock_vec):
    m = np.exp(k * shock_vec)
    # keep same bounds as your code
    m = np.clip(m, 0.6, 1.6)
    return fc.loc[mask2, "HAR"].values * m

y2 = fc.loc[mask2, "RV_next"].values
har2 = fc.loc[mask2, "HAR"].values
shock0 = fc.loc[mask2, "shock"].values

# Test 1: placebo forward shift (+1 day: uses shock_{t+1} to predict RV_{t+1})
shock_plus1 = pd.Series(fc["shock"]).shift(-1).loc[mask2].values
pred_plus1 = llm_from_shock(shock_plus1)

# Test 2: lag shift (-1 day: uses shock_{t-1})
shock_minus1 = pd.Series(fc["shock"]).shift(1).loc[mask2].values
pred_minus1 = llm_from_shock(shock_minus1)

print("\n=== LEAKAGE TESTS (should worsen vs correct alignment) ===")
print("Correct-shock QLIKE:", qlike_mean(y2, llm_from_shock(shock0)))
print("Shock shifted +1  :", qlike_mean(y2, pred_plus1), "  (placebo, should be worse)")
print("Shock shifted -1  :", qlike_mean(y2, pred_minus1), "  (lag, usually worse)")

# Test 3: permutation (shuffle shock across dates)
rng = np.random.default_rng(0)
shuf = shock0.copy()
rng.shuffle(shuf)
pred_shuf = llm_from_shock(shuf)
print("Shock shuffled     :", qlike_mean(y2, pred_shuf), "  (should be close to HAR)")

# Test 4: correlation sanity
rv_t = pd.Series(fc["RV_next"]).shift(1)  # approx RV_t (since RV_next is RV_{t+1})
rv_t = rv_t.loc[mask2].values
def corr(a,b):
    a = np.asarray(a); b = np.asarray(b)
    ok = np.isfinite(a) & np.isfinite(b)
    if ok.sum() < 50: return np.nan
    return float(np.corrcoef(a[ok], b[ok])[0,1])

print("\n=== CORRELATIONS (sanity) ===")
print("corr(shock_t, RV_{t+1}) :", round(corr(shock0, y2), 4))
print("corr(shock_t, RV_{t})   :", round(corr(shock0, rv_t), 4))

print("\nInterpretation:")
print("- If shifted +1 is BETTER than correct -> strong sign of leakage/misalignment.")
print("- If shuffled is still very good -> your improvement isn't truly from news.")

import numpy as np
import pandas as pd

# Load forecasts
fc = pd.read_csv("/content/results/forecasts_H1.csv", parse_dates=["date_t"])
fc = fc.sort_values("date_t").reset_index(drop=True)

# Use your EXISTING merged dataframe
rm = merged.copy()

date_col = "Date" if "Date" in rm.columns else "date_column"
rm[date_col] = pd.to_datetime(rm[date_col]).dt.normalize()

rm_daily = (
    rm[[date_col, "RV", "BVP", "RQ", "ret_d", "ret_abs_d", "ret2_d"]]
    .drop_duplicates(subset=[date_col], keep="last")
    .rename(columns={date_col: "date_t"})
)

df = fc.merge(rm_daily, on="date_t", how="left")

assert df["RV"].notna().mean() > 0.95, "Merge failed — check date alignment"

import numpy as np
import pandas as pd
from sklearn.metrics import roc_auc_score, average_precision_score, precision_recall_curve, brier_score_loss
from sklearn.linear_model import LogisticRegression

# Load forecasts
fc = pd.read_csv("/content/results/forecasts_H1.csv", parse_dates=["date_t"])
fc = fc.sort_values("date_t").reset_index(drop=True)

# Merge realized measures from your merged dataframe
rm = merged.copy()
date_col = "Date" if "Date" in rm.columns else "date_column"
rm[date_col] = pd.to_datetime(rm[date_col]).dt.normalize()

rm_daily = (
    rm[[date_col, "RV", "BVP", "RQ", "ret_d", "ret_abs_d", "ret2_d"]]
    .drop_duplicates(subset=[date_col], keep="last")
    .rename(columns={date_col: "date_t"})
)

df = fc.merge(rm_daily, on="date_t", how="left").sort_values("date_t").reset_index(drop=True)

# ----- Jump definition (standard) -----
df["Jump"] = np.maximum(df["RV"] - df["BVP"], 0.0)

# Expanding quantile threshold tau_q(t) based on past jumps only
q = 0.70
tau = np.full(len(df), np.nan)
for i in range(len(df)):
    past = df.loc[:i-1, "Jump"].values
    if np.isfinite(past).sum() >= 100:
        tau[i] = np.quantile(past[np.isfinite(past)], q)

df["tau_q"] = tau

# Jump event at t+1 (uses Jump_{t+1} compared to tau_q(t) — real-time)
df["Jump_next"] = ((df["Jump"].shift(-1) > df["tau_q"]).astype(float))

# Predictor score (continuous)
df["score"] = np.log(np.clip(df["LLM_pred"].values, 1e-12, None))

# ----- Build a leak-free probability via rolling logistic calibration -----
# p_t = P(Jump_{t+1}=1 | score_t) learned only from past (expanding)
p = np.full(len(df), np.nan)

min_train = 250  # needs enough past days
for i in range(len(df)):
    if i < min_train:
        continue
    # train on past only
    train = df.loc[:i-1, ["score", "Jump_next"]].dropna()
    if len(train) < min_train:
        continue

    Xtr = train[["score"]].values
    ytr = train["Jump_next"].values.astype(int)

    # logistic regression (balanced helps with rare jumps)
    lr = LogisticRegression(class_weight="balanced", solver="lbfgs")
    lr.fit(Xtr, ytr)

    # predict probability for today
    if np.isfinite(df.loc[i, "score"]):
        p[i] = lr.predict_proba([[df.loc[i, "score"]]])[0, 1]

df["p_jump"] = p

# ----- Evaluation (where probability exists) -----
mask = df["Jump_next"].notna() & df["score"].notna()

# AUC/PR-AUC can use the score (rank-based)
y = df.loc[mask, "Jump_next"].astype(int).values
s = df.loc[mask, "score"].values

auc = roc_auc_score(y, s)
pr_auc = average_precision_score(y, s)

# F1 from score threshold
prec, rec, _ = precision_recall_curve(y, s)
f1 = 2 * prec * rec / (prec + rec + 1e-12)
max_f1 = float(np.nanmax(f1))

# Brier uses calibrated probability
mask_brier = df["Jump_next"].notna() & df["p_jump"].notna()
yb = df.loc[mask_brier, "Jump_next"].astype(int).values
pb = df.loc[mask_brier, "p_jump"].clip(0,1).values
brier = float(brier_score_loss(yb, pb))

table4 = pd.DataFrame({
    "Metric": ["AUC (score)", "PR-AUC (score)", "Max F1 (score)", "Brier (prob)"],
    "Value": [float(auc), float(pr_auc), float(max_f1), float(brier)],
    "q_threshold": [q, q, q, q],
    "N_jump_eval": [int(mask.sum()), int(mask.sum()), int(mask.sum()), int(mask_brier.sum())]
})

table4.to_csv("/content/results/paper_tables/Table4_jump_prediction.csv", index=False)
print(table4)

# ----- Table 5: calibration by probability deciles -----
cal = df.loc[mask_brier, ["p_jump", "Jump_next"]].copy()
cal["decile"] = pd.qcut(cal["p_jump"], 5, labels=False, duplicates="drop")

table5 = (
    cal.groupby("decile")
       .agg(Avg_p=("p_jump","mean"),
            Realized_rate=("Jump_next","mean"),
            N=("Jump_next","size"))
       .reset_index()
)

table5.to_csv("/content/results/paper_tables/Table5_jump_calibration.csv", index=False)
print("\n", table5)

import os
import numpy as np
import pandas as pd
import statsmodels.api as sm
from sklearn.metrics import roc_auc_score, average_precision_score, precision_recall_curve, brier_score_loss
from sklearn.linear_model import LogisticRegression

OUT_DIR = "/content/results/paper_tables"
os.makedirs(OUT_DIR, exist_ok=True)

# -----------------------------
# Load forecasts
# -----------------------------
fc = pd.read_csv("/content/results/forecasts_H1.csv", parse_dates=["date_t"])
fc = fc.sort_values("date_t").reset_index(drop=True)

# -----------------------------
# Build realized-measures daily frame from YOUR merged
# -----------------------------
rm = merged.copy()
date_col = "Date" if "Date" in rm.columns else "date_column"
rm[date_col] = pd.to_datetime(rm[date_col], errors="coerce").dt.normalize()

need_cols = [date_col, "RV", "BVP", "RQ", "ret_d", "ret_abs_d", "ret2_d"]
miss = [c for c in need_cols if c not in rm.columns]
if miss:
    raise ValueError(f"merged is missing columns: {miss}. Found: {rm.columns.tolist()}")

rm_daily = (
    rm[need_cols]
    .dropna(subset=[date_col, "RV"])
    .drop_duplicates(subset=[date_col], keep="last")
    .rename(columns={date_col: "date_t"})
    .sort_values("date_t")
    .reset_index(drop=True)
)

df = fc.merge(rm_daily, on="date_t", how="left").sort_values("date_t").reset_index(drop=True)

# sanity
if df["RV"].notna().mean() < 0.90:
    raise RuntimeError("Merge seems misaligned (too many missing RV). Check date_col/timezone alignment.")

# Add standard jump proxy
df["Jump"] = np.maximum(df["RV"] - df["BVP"], 0.0)
df["logRV"] = np.log(np.clip(df["RV"], 1e-12, None))
df["logRV_next"] = np.log(np.clip(df["RV_next"], 1e-12, None))
df["logHAR"] = np.log(np.clip(df["HAR"], 1e-12, None))
df["logHybrid"] = np.log(np.clip(df["Hybrid"], 1e-12, None))
df["logLLM"] = np.log(np.clip(df["LLM_pred"], 1e-12, None))

# ============================================================
# TABLE 1 — Descriptive statistics (recommended in papers)
# ============================================================
def desc_stats(x):
    x = pd.to_numeric(x, errors="coerce")
    x = x[np.isfinite(x)]
    if len(x) == 0:
        return pd.Series({"N":0,"Mean":np.nan,"Std":np.nan,"P5":np.nan,"P50":np.nan,"P95":np.nan})
    return pd.Series({
        "N": int(len(x)),
        "Mean": float(np.mean(x)),
        "Std": float(np.std(x, ddof=1)),
        "P5": float(np.quantile(x, 0.05)),
        "P50": float(np.quantile(x, 0.50)),
        "P95": float(np.quantile(x, 0.95)),
    })

table1 = pd.concat({
    "RV": desc_stats(df["RV"]),
    "BVP": desc_stats(df["BVP"]),
    "Jump=(RV-BVP)+": desc_stats(df["Jump"]),
    "RQ": desc_stats(df["RQ"]),
    "ret_d": desc_stats(df["ret_d"]),
    "ret_abs_d": desc_stats(df["ret_abs_d"]),
    "ret2_d": desc_stats(df["ret2_d"]),
}, axis=1).T.reset_index().rename(columns={"index":"Variable"})

table1.to_csv(f"{OUT_DIR}/Table1_descriptive_stats.csv", index=False)
with open(f"{OUT_DIR}/Table1_descriptive_stats.tex", "w", encoding="utf-8") as f:
    f.write(table1.to_latex(index=False, float_format="%.6f"))

print("Saved Table 1")

# ============================================================
# TABLE 6 — Economic regression (HAC / Newey-West)
# y = log(RV_{t+1}) explained by log forecasts + controls
# ============================================================
reg_df = df.copy()

# Controls: returns + quarticity (as in your dataset)
X = pd.DataFrame({
    "log_HAR": reg_df["logHAR"],
    "log_LLM": reg_df["logLLM"],
    "ret_d": reg_df["ret_d"],
    "ret2_d": reg_df["ret2_d"],
    "log_RQ": np.log(np.clip(reg_df["RQ"], 1e-12, None)),
})

y = reg_df["logRV_next"]

mask_reg = np.isfinite(X).all(axis=1) & np.isfinite(y)
Xr = sm.add_constant(X.loc[mask_reg].astype(float))
yr = y.loc[mask_reg].astype(float)

res = sm.OLS(yr, Xr).fit(cov_type="HAC", cov_kwds={"maxlags":5})

table6 = pd.DataFrame({
    "Coef": res.params,
    "t-stat (HAC)": res.tvalues,
    "p-value": res.pvalues,
})
table6.index.name = "Variable"
table6 = table6.reset_index()

table6.to_csv(f"{OUT_DIR}/Table6_economic_regression.csv", index=False)
with open(f"{OUT_DIR}/Table6_economic_regression.tex", "w", encoding="utf-8") as f:
    f.write(table6.to_latex(index=False, float_format="%.4f"))

print("Saved Table 6")

# ============================================================
# TABLE 7 — Pre/Post COVID performance (HAR vs Hybrid)
# ============================================================
def qlike_mean(y, f):
    y = np.asarray(y, float)
    f = np.clip(np.asarray(f, float), 1e-12, None)
    return float(np.mean(np.log(f) + y/f))

PRE_START, PRE_END = "2014-01-01", "2019-12-31"
POST_START, POST_END = "2020-01-01", "2023-12-31"

def slice_eval(dfs, label):
    m = (dfs["RV_next"] > 0) & (dfs["HAR"] > 0) & (dfs["Hybrid"] > 0)
    ys = dfs.loc[m, "RV_next"].values
    har = dfs.loc[m, "HAR"].values
    hyb = dfs.loc[m, "Hybrid"].values
    if len(ys) < 50:
        return {"Period": label, "N": int(len(ys))}
    har_q = qlike_mean(ys, har)
    hyb_q = qlike_mean(ys, hyb)
    return {
        "Period": label,
        "N": int(len(ys)),
        "HAR QLIKE": har_q,
        "Hybrid QLIKE": hyb_q,
        "Improvement (%)": 100.0 * (har_q - hyb_q) / har_q
    }

pre = df[(df["date_t"] >= PRE_START) & (df["date_t"] <= PRE_END)]
post = df[(df["date_t"] >= POST_START) & (df["date_t"] <= POST_END)]

table7 = pd.DataFrame([
    slice_eval(pre, f"Pre-COVID ({PRE_START[:4]}–{PRE_END[:4]})"),
    slice_eval(post, f"Post-COVID ({POST_START[:4]}–{POST_END[:4]})"),
])

table7.to_csv(f"{OUT_DIR}/Table7_pre_post_covid.csv", index=False)
with open(f"{OUT_DIR}/Table7_pre_post_covid.tex", "w", encoding="utf-8") as f:
    f.write(table7.to_latex(index=False, float_format="%.4f"))

print("Saved Table 7")

# ============================================================
# TABLE 8 — Jump robustness across thresholds q
# Uses: Jump_{t+1} > expanding quantile tau_q(t)
# Score: log(LLM_pred)  (rank metrics)
# Prob: rolling logistic calibration for Brier (leak-free)
# ============================================================
def jump_table_for_q(df_in, q, min_hist=100, min_train=250):
    d = df_in.copy()
    d["Jump"] = np.maximum(d["RV"] - d["BVP"], 0.0)

    tau = np.full(len(d), np.nan)
    for i in range(len(d)):
        past = d.loc[:i-1, "Jump"].values
        past = past[np.isfinite(past)]
        if len(past) >= min_hist:
            tau[i] = np.quantile(past, q)
    d["tau_q"] = tau
    d["Jump_next"] = (d["Jump"].shift(-1) > d["tau_q"]).astype(float)

    d["score"] = np.log(np.clip(d["LLM_pred"].values, 1e-12, None))

    # rank metrics
    mask = d["Jump_next"].notna() & np.isfinite(d["score"])
    y = d.loc[mask, "Jump_next"].astype(int).values
    s = d.loc[mask, "score"].values
    if len(y) < 200:
        return None

    auc = roc_auc_score(y, s)
    pr_auc = average_precision_score(y, s)
    prec, rec, _ = precision_recall_curve(y, s)
    f1 = 2 * prec * rec / (prec + rec + 1e-12)
    max_f1 = float(np.nanmax(f1))

    # leak-free probability via expanding logistic regression
    p = np.full(len(d), np.nan)
    for i in range(len(d)):
        if i < min_train:
            continue
        tr = d.loc[:i-1, ["score","Jump_next"]].dropna()
        if len(tr) < min_train:
            continue
        Xtr = tr[["score"]].values
        ytr = tr["Jump_next"].astype(int).values
        lr = LogisticRegression(class_weight="balanced", solver="lbfgs")
        lr.fit(Xtr, ytr)
        if np.isfinite(d.loc[i, "score"]):
            p[i] = lr.predict_proba([[d.loc[i, "score"]]])[0,1]

    d["p_jump"] = p
    mask_b = d["Jump_next"].notna() & d["p_jump"].notna()
    yb = d.loc[mask_b, "Jump_next"].astype(int).values
    pb = d.loc[mask_b, "p_jump"].clip(0,1).values
    brier = float(brier_score_loss(yb, pb)) if len(yb) > 200 else np.nan

    base_rate = float(np.mean(y))  # unconditional jump rate in eval sample

    return {
        "q": q,
        "N": int(len(y)),
        "Base jump rate": base_rate,
        "AUC": float(auc),
        "PR-AUC": float(pr_auc),
        "Max F1": float(max_f1),
        "Brier": float(brier),
        "N (Brier)": int(len(yb)),
    }

rows = []
for q in [0.60, 0.70, 0.80]:
    r = jump_table_for_q(df, q)
    if r is not None:
        rows.append(r)

table8 = pd.DataFrame(rows)
table8.to_csv(f"{OUT_DIR}/Table8_jump_robustness_q.csv", index=False)
with open(f"{OUT_DIR}/Table8_jump_robustness_q.tex", "w", encoding="utf-8") as f:
    f.write(table8.to_latex(index=False, float_format="%.4f"))

print("Saved Table 8")

# ============================================================
# TABLE 9 — Basel-style capital savings (VaR proxy)
# ============================================================
PORTFOLIO = 100_000_000
Z = 2.326

mcap = (df["HAR"] > 0) & (df["Hybrid"] > 0)
VaR_HAR = Z * np.sqrt(df.loc[mcap, "HAR"].values) * PORTFOLIO
VaR_HYB = Z * np.sqrt(df.loc[mcap, "Hybrid"].values) * PORTFOLIO

table9 = pd.DataFrame([{
    "Portfolio_USD": PORTFOLIO,
    "z_99": Z,
    "Avg_Capital_HAR": float(np.mean(VaR_HAR)),
    "Avg_Capital_Hybrid": float(np.mean(VaR_HYB)),
    "Savings_(HAR-Hybrid)": float(np.mean(VaR_HAR) - np.mean(VaR_HYB)),
    "N": int(mcap.sum())
}])

table9.to_csv(f"{OUT_DIR}/Table9_capital_proxy.csv", index=False)
with open(f"{OUT_DIR}/Table9_capital_proxy.tex", "w", encoding="utf-8") as f:
    f.write(table9.to_latex(index=False, float_format="%.2f"))

print("Saved Table 9")

print("\nDONE. All tables saved to:", OUT_DIR)

# ============================================================
# MULTI-HORIZON (h = 5, 22) — SAME PIPELINE AS YOUR H=1
# Leak-free rolling: HAR vs LLM vs Hybrid + metrics + DM + LaTeX
#
# REQUIREMENT: you already have a DataFrame named `merged`
# with columns including:
#   Date (or date_column), text, RV, BVP, RQ, ret_d, ret_abs_d, ret2_d
#
# OUTPUTS (per horizon h):
#   /content/results/forecasts_H{h}.csv
#   /content/results/metrics_H{h}.csv
#   /content/results/dm_H{h}.csv
#   /content/results/main_results_H{h}.tex
#   /content/results/paper_tables/Table2_forecast_accuracy_H{h}.csv/.tex
#   /content/results/paper_tables/Table3_DM_tests_H{h}.csv/.tex
# ============================================================

import os, re, math, warnings
from dataclasses import dataclass
from collections import deque
from typing import List, Tuple

import numpy as np
import pandas as pd
from tqdm.auto import tqdm
warnings.filterwarnings("ignore")

# ----------------------------
# CONFIG
# ----------------------------
@dataclass
class Cfg:
    col_text: str = "text"
    col_date_pref: str = "Date"
    col_date_alt: str = "date_column"
    col_rv: str = "RV"

    train_win: int = 504
    news_days: int = 60
    topk_context: int = 8

    # reduce LLM calls for long horizons if you want faster:
    llm_every_k_days: int = 3
    llm_retries: int = 1

    cal_window: int = 252
    alpha_window: int = 252

    m_lo: float = 0.60
    m_hi: float = 1.60
    clip_a: float = 0.25
    clip_g: float = 0.30

    alpha_grid: Tuple[float, ...] = tuple(np.round(np.linspace(0, 1, 21), 2))

    max_input_tokens: int = 220
    max_new_tokens: int = 4

    out_dir: str = "/content/results"
    tables_dir: str = "/content/results/paper_tables"

cfg = Cfg()
os.makedirs(cfg.out_dir, exist_ok=True)
os.makedirs(cfg.tables_dir, exist_ok=True)

# ----------------------------
# Sanity: merged must exist
# ----------------------------
try:
    merged  # noqa
except NameError:
    raise RuntimeError("You must have a DataFrame named `merged` loaded.")

# ----------------------------
# Metrics + DM
# ----------------------------
def qlike_mean(y, f):
    y = np.asarray(y, float)
    f = np.clip(np.asarray(f, float), 1e-12, None)
    return float(np.mean(np.log(f) + (y / f)))

def mse_log_mean(y, f):
    y = np.asarray(y, float)
    f = np.asarray(f, float)
    return float(np.mean((np.log(np.clip(y,1e-12,None)) - np.log(np.clip(f,1e-12,None)))**2))

def qlike_loss(y, f):
    y = np.asarray(y, float)
    f = np.clip(np.asarray(f, float), 1e-12, None)
    return np.log(f) + y / f

def dm_test(loss_a, loss_b):
    d = np.asarray(loss_a, float) - np.asarray(loss_b, float)
    d = d[np.isfinite(d)]
    T = len(d)
    if T < 30:
        return {"DM_stat": np.nan, "p_value": np.nan, "N": int(T)}
    dbar = float(np.mean(d))
    var = float(np.var(d, ddof=1)) / T
    dm = dbar / math.sqrt(var + 1e-12)
    from math import erf, sqrt
    cdf = 0.5 * (1.0 + erf(abs(dm)/sqrt(2.0)))
    p = 2.0 * (1.0 - cdf)
    return {"DM_stat": float(dm), "p_value": float(p), "N": int(T)}

# ----------------------------
# Text helpers
# ----------------------------
def clean_text(s: str) -> str:
    return re.sub(r"\s+", " ", str(s)).strip()

def select_bullets_recent_unique(pool_bullets, k: int):
    bullets = [clean_text(b) for b in pool_bullets if isinstance(b, str)]
    bullets = [b for b in bullets if b]
    seen, out = set(), []
    for b in reversed(bullets):
        if b not in seen:
            seen.add(b)
            out.append(b)
        if len(out) >= k:
            break
    return list(reversed(out))

# ----------------------------
# Build DAILY panel once
# ----------------------------
df = merged.copy()
date_col = cfg.col_date_pref if cfg.col_date_pref in df.columns else cfg.col_date_alt
for c in [date_col, cfg.col_text, cfg.col_rv]:
    if c not in df.columns:
        raise ValueError(f"Missing '{c}' in merged. Found: {df.columns.tolist()}")

df[date_col] = pd.to_datetime(df[date_col], errors="coerce").dt.normalize()
df[cfg.col_rv] = pd.to_numeric(df[cfg.col_rv], errors="coerce")
df = df.dropna(subset=[date_col, cfg.col_rv]).copy()
df = df[df[cfg.col_rv] > 0].copy()

daily_rv = (
    df[[date_col, cfg.col_rv]]
    .drop_duplicates(subset=[date_col], keep="last")
    .sort_values(date_col)
    .rename(columns={date_col: "date", cfg.col_rv: "RV"})
    .reset_index(drop=True)
)

daily_news = (
    df[[date_col, cfg.col_text]]
    .dropna(subset=[cfg.col_text])
    .groupby(date_col)[cfg.col_text].apply(list)
    .reset_index(name="news_list")
    .rename(columns={date_col: "date"})
)

daily = daily_rv.merge(daily_news, on="date", how="left")
daily["news_list"] = daily["news_list"].apply(lambda x: x if isinstance(x, list) else [])
daily = daily.sort_values("date").reset_index(drop=True)

daily["logRV"] = np.log(daily["RV"].astype(float))
daily["D"] = daily["logRV"].shift(1)
daily["W"] = daily["logRV"].shift(1).rolling(5, min_periods=5).mean()
daily["M"] = daily["logRV"].shift(1).rolling(22, min_periods=22).mean()
daily = daily.dropna(subset=["D","W","M"]).reset_index(drop=True)

print("Daily base rows:", len(daily), "| range:", daily["date"].min().date(), "→", daily["date"].max().date())

# ----------------------------
# Load LLM once + strict token-level format "d s"
# ----------------------------
import torch
from transformers import AutoTokenizer, AutoModelForCausalLM, LogitsProcessor, LogitsProcessorList

MODEL_NAME = "Qwen/Qwen2.5-1.5B-Instruct"
if not torch.cuda.is_available():
    raise RuntimeError("CUDA GPU required.")

tok = AutoTokenizer.from_pretrained(MODEL_NAME, use_fast=True)
if tok.pad_token_id is None:
    tok.pad_token = tok.eos_token

model = AutoModelForCausalLM.from_pretrained(
    MODEL_NAME,
    device_map={"": 0},
    torch_dtype=torch.float16,
    low_cpu_mem_usage=True
).eval()
torch.set_grad_enabled(False)

def _single_token_id(tokenizer, s: str):
    ids = tokenizer.encode(s, add_special_tokens=False)
    return ids[0] if len(ids) == 1 else None

SPACE_ID = _single_token_id(tok, " ")
EOS_ID = tok.eos_token_id
DIGIT_IDS = {str(i): _single_token_id(tok, str(i)) for i in range(10)}

if SPACE_ID is None or any(v is None for v in DIGIT_IDS.values()):
    raise RuntimeError("Tokenizer digits/space not single-token; strict decoding not supported for this tokenizer.")

DIGITS_0_6 = [DIGIT_IDS[str(i)] for i in range(7)]
DIGITS_0_9 = [DIGIT_IDS[str(i)] for i in range(10)]

class ForceTwoDigitFormat(LogitsProcessor):
    def __init__(self, prompt_len: int, d06: List[int], d09: List[int], space_id: int, eos_id: int):
        self.prompt_len = prompt_len
        self.d06 = d06
        self.d09 = d09
        self.space = space_id
        self.eos = eos_id

    def __call__(self, input_ids, scores):
        step = input_ids.shape[1] - self.prompt_len
        if step == 0:
            allowed = self.d06
        elif step == 1:
            allowed = [self.space]
        elif step == 2:
            allowed = self.d09
        else:
            allowed = [self.eos]
        mask = torch.full_like(scores, float("-inf"))
        mask[:, torch.tensor(allowed, device=scores.device)] = 0.0
        return scores + mask

def llm_generate_two_digits(prompt: str) -> str:
    inputs = tok(prompt, return_tensors="pt", truncation=True, max_length=cfg.max_input_tokens).to(model.device)
    prompt_len = int(inputs["input_ids"].shape[1])
    logits_proc = LogitsProcessorList([ForceTwoDigitFormat(prompt_len, DIGITS_0_6, DIGITS_0_9, SPACE_ID, EOS_ID)])

    with torch.no_grad():
        out = model.generate(
            **inputs,
            max_new_tokens=cfg.max_new_tokens,
            do_sample=False,
            pad_token_id=EOS_ID,
            eos_token_id=EOS_ID,
            logits_processor=logits_proc,
        )

    gen = out[0][prompt_len:]
    txt = tok.decode(gen, skip_special_tokens=True)
    return (txt or "").strip()

DIR_CLASS_TO_DIR = {0:-3, 1:-2, 2:-1, 3:0, 4:1, 5:2, 6:3}

def llm_shock_continuous(h: int, bullets: List[str]):
    bullets_txt = "\n".join([f"- {str(b)[:160]}" for b in bullets]) if bullets else "- (no notable news)"

    # horizon-aware wording (does NOT change algorithm; just clarifies target)
    if h == 5:
        horizon_txt = "next 5 trading days (one week)"
    elif h == 22:
        horizon_txt = "next 22 trading days (about one month)"
    else:
        horizon_txt = f"next {h} trading days"

    SYSTEM = "Return ONLY two digits: d s (one space). No other text."
    USER = f"""
NEWS (up to today):
{bullets_txt}

Task: assess expected volatility over the {horizon_txt}.
Return ONLY: d s

d is direction of volatility change:
0=-3, 1=-2, 2=-1, 3=0, 4=+1, 5=+2, 6=+3
s is strength:
0..9 meaning 0.0..1.0

Examples:
No material info -> 3 0
High uncertainty -> 6 8
Mild uncertainty -> 4 4
Calming news -> 0 6

Output:
""".strip()

    msgs = [{"role":"system","content":SYSTEM},{"role":"user","content":USER}]
    prompt = tok.apply_chat_template(msgs, tokenize=False, add_generation_prompt=True)

    last = ""
    for _ in range(cfg.llm_retries + 1):
        last = llm_generate_two_digits(prompt)
        digs = re.findall(r"\b(\d)\b", last)
        if len(digs) >= 2:
            d = int(digs[0]); s = int(digs[1])
            if 0 <= d <= 6 and 0 <= s <= 9:
                direction = DIR_CLASS_TO_DIR[d]
                strength = s / 9.0
                return float(direction) * float(strength), True, last
    return 0.0, False, last

# ----------------------------
# HAR + robust calibration + alpha selection
# ----------------------------
def fit_har(train_df: pd.DataFrame):
    X = np.column_stack([np.ones(len(train_df)), train_df["D"].values, train_df["W"].values, train_df["M"].values])
    y = train_df["logRV_target"].values
    beta, *_ = np.linalg.lstsq(X, y, rcond=None)
    return beta

def har_forecast(beta, row: pd.Series) -> float:
    x = np.array([1.0, row["D"], row["W"], row["M"]], float)
    return float(np.exp(x @ beta))

def estimate_a_g_huber(pairs: List[Tuple[float, float]]):
    if len(pairs) < 80:
        return 0.0, 0.05
    s = np.array([p[0] for p in pairs], float)
    r = np.array([p[1] for p in pairs], float)
    X = np.column_stack([np.ones(len(s)), s])

    beta, *_ = np.linalg.lstsq(X, r, rcond=None)
    k = 1.345
    for _ in range(12):
        resid = r - (X @ beta)
        scale = np.median(np.abs(resid)) / 0.6745 + 1e-8
        u = resid / scale
        w = np.ones_like(u)
        m = np.abs(u) > k
        w[m] = k / np.abs(u[m])
        W = w[:, None]
        beta_new, *_ = np.linalg.lstsq(X * W, r * w, rcond=None)
        if np.max(np.abs(beta_new - beta)) < 1e-6:
            beta = beta_new
            break
        beta = beta_new

    a = float(np.clip(beta[0], -cfg.clip_a, cfg.clip_a))
    g = float(np.clip(beta[1], -cfg.clip_g, cfg.clip_g))
    return a, g

def choose_alpha(y_hist, har_hist, llm_hist):
    if len(y_hist) < 80:
        return 0.5
    best_a, best = 0.5, float("inf")
    for a in cfg.alpha_grid:
        hyb = np.exp(a*np.log(np.clip(har_hist,1e-12,None)) + (1-a)*np.log(np.clip(llm_hist,1e-12,None)))
        loss = qlike_mean(y_hist, hyb)
        if loss < best:
            best = loss
            best_a = a
    return float(best_a)

# ----------------------------
# Run one horizon (h)
# ----------------------------
def run_horizon(h: int):
    d = daily.copy()
    d["RV_target"] = d["RV"].shift(-h)
    d = d.iloc[:-h].reset_index(drop=True)
    d["logRV_target"] = np.log(np.clip(d["RV_target"], 1e-12, None))

    print(f"\nH{h}: rows after target alignment:", len(d),
          "| range:", d["date"].min().date(), "→", d["date"].max().date())

    # News pool
    pool = deque()
    def add_day_news(i: int):
        dt = d.loc[i, "date"]
        for b in d.loc[i, "news_list"]:
            b = clean_text(b)
            if b:
                pool.append((dt, b))
    def evict_old(cur_date):
        cutoff = cur_date - pd.Timedelta(days=cfg.news_days)
        while pool and pool[0][0] < cutoff:
            pool.popleft()

    warm = min(len(d), cfg.train_win + 10)
    for i in range(warm):
        add_day_news(i)

    # rolling calibration + alpha
    ag_pairs = deque(maxlen=cfg.cal_window)
    y_hist = deque(maxlen=cfg.alpha_window)
    har_hist = deque(maxlen=cfg.alpha_window)
    llm_hist = deque(maxlen=cfg.alpha_window)

    last_valid_shock = 0.0
    LLM_CALLS = 0
    FAIL_CALLS = 0
    records = []

    end_t = len(d) - 1

    for t in tqdm(range(cfg.train_win, end_t), desc=f"MAIN H={h} (rolling)"):
        info_date = d.loc[t, "date"]
        evict_old(info_date)
        bullets = select_bullets_recent_unique([b for (_, b) in pool], k=cfg.topk_context)

        # HAR fit on past-only window
        tr = d.iloc[t-cfg.train_win:t].copy()
        beta = fit_har(tr)
        har_pred = har_forecast(beta, d.loc[t])

        # LLM shock (cached between calls)
        call_today = ((t - cfg.train_win) % cfg.llm_every_k_days == 0) or (t == cfg.train_win)
        raw_out = ""
        if call_today:
            shock, ok, raw_out = llm_shock_continuous(h, bullets)
            LLM_CALLS += 1
            if ok:
                last_valid_shock = shock
            else:
                FAIL_CALLS += 1
                shock = last_valid_shock
        else:
            shock = last_valid_shock
            ok = True

        # Robust calibration m = exp(a + g*shock) on past-only pairs
        a, g = estimate_a_g_huber(list(ag_pairs))
        m = float(np.exp(a + g * float(shock)))
        m = float(np.clip(m, cfg.m_lo, cfg.m_hi))
        llm_pred = float(har_pred * m)

        # Time-varying alpha on past-only window
        alpha_t = choose_alpha(np.array(y_hist), np.array(har_hist), np.array(llm_hist))
        hybrid = float(np.exp(alpha_t*np.log(max(har_pred,1e-12)) + (1-alpha_t)*np.log(max(llm_pred,1e-12))))

        y_true = float(d.loc[t, "RV_target"])
        log_ratio = float(np.log(max(y_true,1e-12)) - np.log(max(har_pred,1e-12)))

        ag_pairs.append((float(shock), log_ratio))
        y_hist.append(y_true); har_hist.append(har_pred); llm_hist.append(llm_pred)

        records.append({
            "h": h,
            "date_t": info_date,
            "RV_target": y_true,
            "HAR": har_pred,
            "shock": float(shock),
            "a": float(a),
            "g": float(g),
            "m": float(m),
            "LLM_pred": llm_pred,
            "alpha_t": float(alpha_t),
            "Hybrid": hybrid,
            "LLM_called_today": int(call_today),
            "LLM_call_failed": int(call_today and (not ok)),
            "LLM_raw_out": raw_out[:10],
        })

        if t + 1 < len(d):
            add_day_news(t + 1)

    fc = pd.DataFrame(records)
    fc["date_t"] = pd.to_datetime(fc["date_t"]).dt.normalize()

    # Save forecasts
    PATH_FC  = os.path.join(cfg.out_dir, f"forecasts_H{h}.csv")
    PATH_MET = os.path.join(cfg.out_dir, f"metrics_H{h}.csv")
    PATH_DM  = os.path.join(cfg.out_dir, f"dm_H{h}.csv")
    PATH_TEX = os.path.join(cfg.out_dir, f"main_results_H{h}.tex")
    fc.to_csv(PATH_FC, index=False)

    # Metrics + DM
    mask = np.isfinite(fc["RV_target"]) & (fc["RV_target"] > 0)
    for c in ["HAR","LLM_pred","Hybrid"]:
        mask &= np.isfinite(fc[c]) & (fc[c] > 0)

    y   = fc.loc[mask, "RV_target"].values
    har = fc.loc[mask, "HAR"].values
    llm = fc.loc[mask, "LLM_pred"].values
    hyb = fc.loc[mask, "Hybrid"].values

    fallback_rate = float(fc.loc[fc["LLM_called_today"]==1, "LLM_call_failed"].mean()) if fc["LLM_called_today"].sum() else 0.0

    met = pd.DataFrame([
        {"model":"HAR",    "N":int(mask.sum()), "QLIKE": qlike_mean(y, har), "MSElog": mse_log_mean(y, har)},
        {"model":"LLM",    "N":int(mask.sum()), "QLIKE": qlike_mean(y, llm), "MSElog": mse_log_mean(y, llm)},
        {"model":"Hybrid", "N":int(mask.sum()), "QLIKE": qlike_mean(y, hyb), "MSElog": mse_log_mean(y, hyb)},
    ])
    met["h"] = h
    met["model_name"] = MODEL_NAME
    met["llm_every_k_days"] = cfg.llm_every_k_days
    met["news_days"] = cfg.news_days
    met["topk_context"] = cfg.topk_context
    met["LLM_calls"] = int(fc["LLM_called_today"].sum())
    met["fallback_rate_among_calls"] = fallback_rate
    met["cal_window"] = cfg.cal_window
    met["m_bounds"] = f"[{cfg.m_lo},{cfg.m_hi}]"
    met["alpha_window"] = cfg.alpha_window
    met.to_csv(PATH_MET, index=False)

    dm_df = pd.DataFrame([
        {"comparison":"HAR vs LLM",    **dm_test(qlike_loss(y, har), qlike_loss(y, llm))},
        {"comparison":"HAR vs Hybrid", **dm_test(qlike_loss(y, har), qlike_loss(y, hyb))},
        {"comparison":"LLM vs Hybrid", **dm_test(qlike_loss(y, llm), qlike_loss(y, hyb))},
    ])
    dm_df.insert(0, "h", h)
    dm_df.to_csv(PATH_DM, index=False)

    with open(PATH_TEX, "w", encoding="utf-8") as f:
        f.write(met[["model","N","QLIKE","MSElog","fallback_rate_among_calls","LLM_calls"]].to_latex(
            index=False, float_format="%.4f"
        ))

    # Paper-style tables (Table 2 & 3) for this horizon
    t2 = met[["model","N","QLIKE","MSElog"]].copy()
    t3 = dm_df[["h","comparison","DM_stat","p_value","N"]].copy()

    t2_csv = os.path.join(cfg.tables_dir, f"Table2_forecast_accuracy_H{h}.csv")
    t2_tex = os.path.join(cfg.tables_dir, f"Table2_forecast_accuracy_H{h}.tex")
    t3_csv = os.path.join(cfg.tables_dir, f"Table3_DM_tests_H{h}.csv")
    t3_tex = os.path.join(cfg.tables_dir, f"Table3_DM_tests_H{h}.tex")

    t2.to_csv(t2_csv, index=False)
    t3.to_csv(t3_csv, index=False)
    with open(t2_tex, "w", encoding="utf-8") as f:
        f.write(t2.to_latex(index=False, float_format="%.4f"))
    with open(t3_tex, "w", encoding="utf-8") as f:
        f.write(t3.to_latex(index=False, float_format="%.4f"))

    print(f"\n==================== RESULTS SAVED (H{h}) ====================")
    print("Forecasts:", PATH_FC)
    print("Metrics  :", PATH_MET)
    print("DM tests :", PATH_DM)
    print("LaTeX    :", PATH_TEX)
    print("\n===== METRICS (H{}) =====".format(h))
    print(met)
    print("\n===== DM TESTS (H{}) =====".format(h))
    print(dm_df)
    print("\nAUDIT:")
    print("MODEL:", MODEL_NAME)
    print("LLM_CALLS:", int(fc["LLM_called_today"].sum()))
    print("Fallback rate among calls:", round(100*fallback_rate, 3), "%")

    return met, dm_df

# ----------------------------
# RUN horizons 5 and 22
# ----------------------------
metrics_all = []
dm_all = []

for h in [5, 22]:
    met_h, dm_h = run_horizon(h)
    metrics_all.append(met_h)
    dm_all.append(dm_h)

metrics_all = pd.concat(metrics_all, ignore_index=True)
dm_all = pd.concat(dm_all, ignore_index=True)

metrics_all.to_csv(os.path.join(cfg.out_dir, "metrics_H5_H22.csv"), index=False)
dm_all.to_csv(os.path.join(cfg.out_dir, "dm_H5_H22.csv"), index=False)

print("\n==================== COMBINED SAVED ====================")
print("Combined metrics:", os.path.join(cfg.out_dir, "metrics_H5_H22.csv"))
print("Combined DM     :", os.path.join(cfg.out_dir, "dm_H5_H22.csv"))
print("Paper tables dir:", cfg.tables_dir)

import numpy as np
import pandas as pd

from sklearn.linear_model import SGDClassifier
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import (
    roc_auc_score, average_precision_score,
    roc_curve, precision_recall_curve,
    brier_score_loss
)

# -----------------------------
# 1) Load H1 forecasts (daily)
# -----------------------------
fc = pd.read_csv("/content/results/forecasts_H1.csv", parse_dates=["date_t"])
fc["date_t"] = pd.to_datetime(fc["date_t"]).dt.normalize()
fc = fc.sort_values("date_t").reset_index(drop=True)

# -----------------------------
# 2) Make merged DAILY UNIQUE (this is the key fix)
# -----------------------------
rm = merged.copy()

# pick the correct date column
if "Date" in rm.columns:
    rm["Date"] = pd.to_datetime(rm["Date"]).dt.normalize()
    dcol = "Date"
elif "date_column" in rm.columns:
    rm["date_column"] = pd.to_datetime(rm["date_column"]).dt.normalize()
    dcol = "date_column"
else:
    raise ValueError("merged must contain 'Date' or 'date_column'.")

need = ["RQ", "ret_d", "ret2_d"]
miss = [c for c in need if c not in rm.columns]
if miss:
    raise ValueError(f"merged is missing required columns for jump table: {miss}")

# IMPORTANT: collapse to one row per day
# For daily measures, 'first' is fine if they are identical within date;
# if not identical, you probably want 'mean' or 'last'. Use 'first' by default.
rm_daily = (
    rm.sort_values(dcol)
      .groupby(dcol, as_index=False)[need]
      .first()
      .rename(columns={dcol: "date_t"})
)

rm_daily["date_t"] = pd.to_datetime(rm_daily["date_t"]).dt.normalize()

# sanity check: 1 row per day
if rm_daily["date_t"].duplicated().any():
    raise RuntimeError("rm_daily still has duplicate dates — something is wrong with grouping.")

# -----------------------------
# 3) Merge daily controls into daily forecasts (1:1)
# -----------------------------
df = fc.merge(rm_daily, on="date_t", how="inner").sort_values("date_t").reset_index(drop=True)

# -----------------------------
# 4) Build jump label at t+1 using RQ_next > rolling quantile
# -----------------------------
q = 0.70
roll = 252

df["RQ_next"] = df["RQ"].shift(-1)
df["rq_q"] = df["RQ_next"].rolling(roll).quantile(q)

df = df.dropna(subset=["RQ", "RQ_next", "rq_q", "ret_d", "ret2_d", "shock", "log_ratio"]).copy()
df["jump_next"] = (df["RQ_next"] > df["rq_q"]).astype(int)

# Features at time t (no leakage)
df["log_RQ"] = np.log(np.maximum(df["RQ"].astype(float), 1e-12))
feat_cols = ["shock", "log_ratio", "ret_d", "ret2_d", "log_RQ"]
X = df[feat_cols].astype(float).values
y = df["jump_next"].astype(int).values

# -----------------------------
# 5) Online logit (fast) with manual class balancing
# -----------------------------
train_len = 504
if len(df) <= train_len + 10:
    raise ValueError(f"Not enough DAILY rows after alignment: {len(df)}")

scaler = StandardScaler()
X0 = scaler.fit_transform(X[:train_len])
y0 = y[:train_len]

pos = (y0 == 1).sum()
neg = (y0 == 0).sum()
if pos == 0 or neg == 0:
    raise ValueError("Initial training window has only one class. Increase train_len or adjust jump definition.")

w_pos = len(y0) / (2 * pos)
w_neg = len(y0) / (2 * neg)

def weights_for(y_vec):
    return np.where(y_vec == 1, w_pos, w_neg).astype(float)

clf = SGDClassifier(loss="log_loss", alpha=1e-4, max_iter=1, tol=None, random_state=0)
clf.partial_fit(X0, y0, classes=np.array([0,1]), sample_weight=weights_for(y0))

p_jump = np.full(len(df), np.nan)

for i in range(train_len, len(df)):
    xi = scaler.transform(X[i:i+1])
    p_jump[i] = clf.predict_proba(xi)[0,1]
    yi = y[i:i+1]
    clf.partial_fit(xi, yi, sample_weight=weights_for(yi))

df["p_jump"] = p_jump

# evaluation subset
ev = df.dropna(subset=["p_jump"]).copy()
y_ev = ev["jump_next"].astype(int).values
p_ev = np.clip(ev["p_jump"].astype(float).values, 1e-6, 1-1e-6)

# -----------------------------
# 6) Table 4 metrics + two methods
# -----------------------------
auc = roc_auc_score(y_ev, p_ev)
pr_auc = average_precision_score(y_ev, p_ev)
brier = brier_score_loss(y_ev, p_ev)

# Youden-J threshold
fpr, tpr, thr_roc = roc_curve(y_ev, p_ev)
thr_J = thr_roc[np.argmax(tpr - fpr)]

# F1-optimal threshold
prec, rec, thr_pr = precision_recall_curve(y_ev, p_ev)
f1 = 2*prec*rec/(prec+rec+1e-12)
thr_F1 = thr_pr[np.nanargmax(f1)]

def eval_at(thr):
    pred = (p_ev >= thr).astype(int)
    TP = ((pred==1)&(y_ev==1)).sum()
    FP = ((pred==1)&(y_ev==0)).sum()
    FN = ((pred==0)&(y_ev==1)).sum()
    recall = TP/(TP+FN+1e-12)
    precision = TP/(TP+FP+1e-12)
    f1v = 2*precision*recall/(precision+recall+1e-12)
    return recall, precision, f1v

rec_J, prec_J, f1_J = eval_at(thr_J)
rec_F, prec_F, f1_F = eval_at(thr_F1)

table4 = pd.DataFrame({
    "Method": ["Youden-J", "F1-optimal"],
    "Threshold": [thr_J, thr_F1],
    "Recall": [rec_J, rec_F],
    "Precision": [prec_J, prec_F],
    "F1": [f1_J, f1_F],
    "AUC": [auc, auc],
    "PR-AUC": [pr_auc, pr_auc],
    "Brier": [brier, brier],
    "q_threshold": [q, q],
    "N_jump_eval": [len(ev), len(ev)]
})

# Save outputs
out_pred = "/content/results/jump_predictions_H1.csv"
out_csv  = "/content/results/paper_tables/Table4_jump_prediction.csv"
out_tex  = "/content/results/paper_tables/Table4_jump_prediction.tex"

ev[["date_t","jump_next","p_jump"]].to_csv(out_pred, index=False)
table4.to_csv(out_csv, index=False)
table4.to_latex(out_tex, index=False, float_format="%.3f")

print("DAILY rows in ev:", len(ev))
print(table4)
print("\nSaved:")
print(out_pred)
print(out_csv)
print(out_tex)

# ============================================================
# H1 ONLY: Generate Fig 5, Fig 6, and Table 6 (Economic Drivers)
# WITHOUT changing your forecasting method.
#
# Inputs required:
#  1) /content/results/forecasts_H1.csv  (your existing forecast output)
#     must contain: ['date_t','shock','LLM_called_today'] at least
#  2) `merged` DataFrame in memory (your daily merged dataset)
#     must contain: Date (or date_column) and text
#
# What this script does:
#  A) Builds daily news bullets from merged['text']
#  B) For each day where LLM was called (LLM_called_today==1), runs ONE LLM call
#     to produce: drivers + rationale in STRICT JSON
#     (this does NOT affect forecasts at all)
#  C) Saves an updated file:
#     /content/results/forecasts_H1_with_rationales.csv
#  D) Creates:
#     Table 6: /content/results/paper_tables/Table6_economic_drivers_H1.csv + .tex
#     Fig 5  : /content/results/paper_figures/Fig5_wordcloud_rationales_H1.png
#     Fig 6  : /content/results/paper_figures/Fig6_driver_counts_H1.png
#
# ============================================================

import os, re, json, warnings
import numpy as np
import pandas as pd

warnings.filterwarnings("ignore")

# -----------------------------
# Paths
# -----------------------------
FORECASTS_IN  = "/content/results/forecasts_H1.csv"
FORECASTS_OUT = "/content/results/forecasts_H1_with_rationales.csv"

TABLES_DIR = "/content/results/paper_tables"
FIGS_DIR   = "/content/results/paper_figures"
os.makedirs(TABLES_DIR, exist_ok=True)
os.makedirs(FIGS_DIR, exist_ok=True)

TABLE6_CSV = f"{TABLES_DIR}/Table6_economic_drivers_H1.csv"
TABLE6_TEX = f"{TABLES_DIR}/Table6_economic_drivers_H1.tex"
FIG5_PATH  = f"{FIGS_DIR}/Fig5_wordcloud_rationales_H1.png"
FIG6_PATH  = f"{FIGS_DIR}/Fig6_driver_counts_H1.png"

# -----------------------------
# Load forecasts
# -----------------------------
fc = pd.read_csv(FORECASTS_IN, parse_dates=["date_t"])
required_cols = ["date_t", "shock", "LLM_called_today"]
missing = [c for c in required_cols if c not in fc.columns]
if missing:
    raise ValueError(f"forecasts_H1.csv missing required columns: {missing}. Found: {fc.columns.tolist()}")

fc["date_t"] = pd.to_datetime(fc["date_t"]).dt.normalize()
fc["shock"] = pd.to_numeric(fc["shock"], errors="coerce")

# -----------------------------
# Sanity: merged must exist
# -----------------------------
try:
    merged  # noqa
except NameError:
    raise RuntimeError("You must have a DataFrame named `merged` in memory (your daily merged dataset).")

rm = merged.copy()
date_col = "Date" if "Date" in rm.columns else ("date_column" if "date_column" in rm.columns else None)
if date_col is None:
    raise ValueError("merged must contain 'Date' or 'date_column'.")
if "text" not in rm.columns:
    raise ValueError("merged must contain 'text' column (news text).")

rm[date_col] = pd.to_datetime(rm[date_col], errors="coerce").dt.normalize()

# -----------------------------
# Build daily news text + bullets
# -----------------------------
def make_bullets(day_text: str, k: int = 8):
    if not isinstance(day_text, str) or not day_text.strip():
        return []
    parts = re.split(r"[\n\r]+|(?<=[.!?])\s+", day_text.strip())
    parts = [p.strip(" -•\t") for p in parts if len(p.strip()) > 20]
    seen, out = set(), []
    for p in parts:
        key = p.lower()
        if key in seen:
            continue
        seen.add(key)
        out.append(p)
        if len(out) >= k:
            break
    return out

daily_news = (
    rm.dropna(subset=["text"])
      .groupby(date_col)["text"]
      .apply(lambda x: " ".join(map(str, x)))
      .reset_index()
      .rename(columns={date_col: "date_t", "text": "news_text"})
)
daily_news["date_t"] = pd.to_datetime(daily_news["date_t"]).dt.normalize()
daily_news["bullets"] = daily_news["news_text"].apply(lambda x: make_bullets(x, k=8))

# Merge bullets into fc
df = fc.merge(daily_news[["date_t", "bullets"]], on="date_t", how="left")
df["bullets"] = df["bullets"].apply(lambda x: x if isinstance(x, list) else [])

# -----------------------------
# LLM setup (for rationales/drivers only)
# -----------------------------
import torch
from transformers import AutoTokenizer, AutoModelForCausalLM

MODEL_NAME = "Qwen/Qwen2.5-1.5B-Instruct"
if not torch.cuda.is_available():
    raise RuntimeError("CUDA GPU required for fast LLM inference (as in your runs).")

tok = AutoTokenizer.from_pretrained(MODEL_NAME, use_fast=True)
if tok.pad_token_id is None:
    tok.pad_token = tok.eos_token

model = AutoModelForCausalLM.from_pretrained(
    MODEL_NAME,
    device_map={"": 0},
    torch_dtype=torch.float16,
    low_cpu_mem_usage=True
).eval()
torch.set_grad_enabled(False)

DRIVER_SET = ["Inventory", "Demand", "Jobs", "Rates", "Sanctions"]

def _extract_first_json(txt: str):
    m = re.search(r"\{.*\}", txt, flags=re.DOTALL)
    if not m:
        return None
    try:
        return json.loads(m.group(0))
    except Exception:
        return None

def llm_generate(prompt: str, max_new_tokens: int = 160) -> str:
    inputs = tok(prompt, return_tensors="pt", truncation=True, max_length=700).to(model.device)
    with torch.no_grad():
        out = model.generate(
            **inputs,
            do_sample=False,
            max_new_tokens=max_new_tokens,
            eos_token_id=tok.eos_token_id,
            pad_token_id=tok.eos_token_id
        )
    gen = out[0][inputs["input_ids"].shape[1]:]
    return tok.decode(gen, skip_special_tokens=True).strip()

def llm_rationale_drivers(info_date: str, shock_value: float, bullets: list, retries: int = 2):
    bullets_txt = "\n".join([f"- {str(b)[:180]}" for b in bullets[:8]]) if bullets else "- (no notable news)"
    # Note: we give the already-produced shock to the LLM and ask it to justify it.
    # This does NOT change forecasting; it's a post-hoc explanation for interpretability in the paper.
    system = "Return STRICT JSON only. No other text."
    user = f"""
Information date: {info_date}. Use only these news bullets (no future info).

We have an LLM volatility shock decision for next-day volatility: shock = {shock_value}.
Explain why this shock makes sense and tag the key economic drivers.

Return JSON with exactly these keys:
- "drivers": list of 0-3 items chosen ONLY from {DRIVER_SET}
- "rationale": 1-2 sentences (plain English). Mention the most relevant drivers.

Rules:
- drivers must be chosen from the allowed list exactly (case-sensitive).
- If the news is unclear/mixed: drivers=[], rationale should say mixed/unclear.

News bullets:
{bullets_txt}
""".strip()

    msgs = [{"role": "system", "content": system}, {"role": "user", "content": user}]
    prompt = tok.apply_chat_template(msgs, tokenize=False, add_generation_prompt=True)

    raw = ""
    for _ in range(retries + 1):
        raw = llm_generate(prompt, max_new_tokens=160)
        obj = _extract_first_json(raw)
        if isinstance(obj, dict) and ("drivers" in obj) and ("rationale" in obj):
            drivers = obj.get("drivers", [])
            if drivers is None:
                drivers = []
            if not isinstance(drivers, list):
                drivers = []
            drivers = [d for d in drivers if d in DRIVER_SET][:3]
            rationale = str(obj.get("rationale", "")).strip()
            if len(rationale) >= 5:
                return drivers, rationale, 0, raw
        prompt += "\n\nREMINDER: Output MUST be valid JSON only."
    return [], "", 1, raw

# -----------------------------
# Generate rationales only for days where LLM was actually called
# -----------------------------
mask_call = (df["LLM_called_today"] == 1)
call_idx = df.index[mask_call].tolist()

# Precreate columns
df["LLM_drivers"] = ""
df["LLM_rationale"] = ""
df["LLM_rationale_failed"] = 0
df["LLM_rationale_raw"] = ""

for k in DRIVER_SET:
    df[f"drv_{k}"] = 0

print("Days with LLM_called_today==1:", len(call_idx))

from tqdm.auto import tqdm
for i in tqdm(call_idx, desc="Generating rationales/drivers (H1)"):
    date_t = df.at[i, "date_t"]
    shock = df.at[i, "shock"]
    bullets = df.at[i, "bullets"]
    drivers, rationale, failed, raw = llm_rationale_drivers(
        info_date=str(pd.to_datetime(date_t).date()),
        shock_value=float(shock) if np.isfinite(shock) else 0.0,
        bullets=bullets,
        retries=2
    )
    df.at[i, "LLM_drivers"] = ",".join(drivers) if drivers else ""
    df.at[i, "LLM_rationale"] = rationale
    df.at[i, "LLM_rationale_failed"] = int(failed)
    df.at[i, "LLM_rationale_raw"] = raw[:500]  # keep short

    s = set(drivers)
    for dname in DRIVER_SET:
        df.at[i, f"drv_{dname}"] = int(dname in s)

# Carry forward last rationale on non-call days (optional, helps Fig5 continuity)
last_drivers = ""
last_rationale = ""
for i in df.index:
    if df.at[i, "LLM_called_today"] == 1:
        if df.at[i, "LLM_rationale"]:
            last_rationale = df.at[i, "LLM_rationale"]
            last_drivers = df.at[i, "LLM_drivers"]
    else:
        df.at[i, "LLM_rationale"] = last_rationale
        df.at[i, "LLM_drivers"] = last_drivers
        # drv_* left as 0 for non-call days (since no new tags)

# Save updated forecasts with rationales
df.drop(columns=["bullets"], errors="ignore").to_csv(FORECASTS_OUT, index=False)

fail_rate = df.loc[df["LLM_called_today"]==1, "LLM_rationale_failed"].mean()
print("\nSaved:", FORECASTS_OUT)
print("Rationale parse fail rate among call days:", round(100*fail_rate, 3), "%")

# -----------------------------
# TABLE 6: Economic drivers regression
# log(RV_next) ~ log(LLM_pred) + driver dummies
# -----------------------------
import statsmodels.api as sm

need = ["RV_next", "LLM_pred"] + [f"drv_{k}" for k in DRIVER_SET]
miss = [c for c in need if c not in df.columns]
if miss:
    raise ValueError(f"Missing columns for Table 6 regression: {miss}")

reg = df.copy()
reg = reg[(reg["RV_next"] > 0) & (reg["LLM_pred"] > 0)].copy()
reg["log_RV_next"] = np.log(np.maximum(reg["RV_next"].astype(float), 1e-12))
reg["log_LLM_pred"] = np.log(np.maximum(reg["LLM_pred"].astype(float), 1e-12))

Xcols = ["log_LLM_pred"] + [f"drv_{k}" for k in DRIVER_SET]
X = sm.add_constant(reg[Xcols].astype(float))
y = reg["log_RV_next"].astype(float)

ols = sm.OLS(y, X).fit(cov_type="HC1")

def stars(p):
    if p < 0.01: return "***"
    if p < 0.05: return "**"
    if p < 0.10: return "*"
    return ""

rows = []
rows.append({"Feature":"Constant", "Coefficient": float(ols.params["const"]), "p_value": float(ols.pvalues["const"]), "sig": stars(float(ols.pvalues["const"]))})
rows.append({"Feature":"LLM Log-Forecast", "Coefficient": float(ols.params["log_LLM_pred"]), "p_value": float(ols.pvalues["log_LLM_pred"]), "sig": stars(float(ols.pvalues["log_LLM_pred"]))})
for k in DRIVER_SET:
    col = f"drv_{k}"
    rows.append({"Feature":k, "Coefficient": float(ols.params[col]), "p_value": float(ols.pvalues[col]), "sig": stars(float(ols.pvalues[col]))})

t6 = pd.DataFrame(rows)
t6.to_csv(TABLE6_CSV, index=False)

t6_disp = t6.copy()
t6_disp["Coefficient"] = t6_disp["Coefficient"].map(lambda v: f"{v:.4f}") + t6_disp["sig"]
t6_disp["p_value"] = t6_disp["p_value"].map(lambda v: f"{v:.3g}")
t6_disp = t6_disp.drop(columns=["sig"])

with open(TABLE6_TEX, "w", encoding="utf-8") as f:
    f.write(t6_disp.to_latex(index=False))

print("\nSaved Table 6:")
print(TABLE6_CSV)
print(TABLE6_TEX)
print("\nTable 6 preview:")
print(t6_disp)

# -----------------------------
# FIG 6: Driver counts (bar chart)
# -----------------------------
import matplotlib.pyplot as plt

call_days = df[df["LLM_called_today"]==1].copy()
drv_counts = {k: int(call_days[f"drv_{k}"].sum()) for k in DRIVER_SET}

plt.figure(figsize=(8,4))
plt.bar(list(drv_counts.keys()), list(drv_counts.values()))
plt.xticks(rotation=25, ha="right")
plt.ylabel("Frequency (LLM-called days)")
plt.title("Figure 6: Top economic drivers in LLM rationales")
plt.tight_layout()
plt.savefig(FIG6_PATH, dpi=200)
plt.show()

print("\nSaved Fig 6:", FIG6_PATH)

# -----------------------------
# FIG 5: Word cloud of rationales
# -----------------------------
text_blob = " ".join([t for t in call_days["LLM_rationale"].astype(str).tolist() if t.strip()])
text_blob = text_blob.lower()
text_blob = re.sub(r"\b\d+\b", " ", text_blob)
text_blob = re.sub(r"[^a-z\s]", " ", text_blob)
text_blob = re.sub(r"\s+", " ", text_blob).strip()

try:
    from wordcloud import WordCloud, STOPWORDS
    custom_stop = set(STOPWORDS).union({"oil","market","price","prices","volatility","news","today","expected","risk"})
    wc = WordCloud(width=1200, height=650, background_color="white",
                   stopwords=custom_stop, collocations=False).generate(text_blob)

    plt.figure(figsize=(10,5))
    plt.imshow(wc, interpolation="bilinear")
    plt.axis("off")
    plt.title("Figure 5: Word cloud of LLM rationales (H1)")
    plt.tight_layout()
    plt.savefig(FIG5_PATH, dpi=200)
    plt.show()
    print("\nSaved Fig 5:", FIG5_PATH)
except Exception as e:
    # If wordcloud isn't installed: fallback to top-word bar plot
    words = re.findall(r"[a-z]{4,}", text_blob)
    stop = set(["this","that","with","from","have","will","more","than","over","into","also","said"])
    words = [w for w in words if w not in stop]
    vc = pd.Series(words).value_counts().head(25)

    plt.figure(figsize=(10,4))
    plt.bar(vc.index.tolist(), vc.values.tolist())
    plt.xticks(rotation=60, ha="right")
    plt.ylabel("Count")
    plt.title("Figure 5 (fallback): Top terms in LLM rationales (H1)")
    plt.tight_layout()
    plt.savefig(FIG5_PATH, dpi=200)
    plt.show()
    print("\nSaved Fig 5 (fallback plot):", FIG5_PATH)
    print("Note: install wordcloud to get a true word cloud:  pip install wordcloud")

print("\nDONE.")
print("Updated forecasts with rationales:", FORECASTS_OUT)
print("Table 6:", TABLE6_CSV)
print("Fig 5:", FIG5_PATH)
print("Fig 6:", FIG6_PATH)

import pandas as pd
import matplotlib.pyplot as plt
import re
from collections import Counter

# ----------------------------------------
# Load forecasts WITH rationales
# ----------------------------------------
df = pd.read_csv(
    "/content/results/forecasts_H1_with_rationales.csv",
    parse_dates=["date_t"]
)

# Only LLM call days
df = df[df["LLM_called_today"] == 1].copy()

# ----------------------------------------
# Collect rationale text
# ----------------------------------------
text_all = " ".join(df["LLM_rationale"].dropna().astype(str)).lower()

# ----------------------------------------
# Strong text cleaning
# ----------------------------------------
text_all = re.sub(r"\b\d+\b", " ", text_all)
text_all = re.sub(r"[^a-z\s]", " ", text_all)
text_all = re.sub(r"\s+", " ", text_all).strip()

# ----------------------------------------
# ECONOMIC stopwords (custom + generic)
# ----------------------------------------
stopwords = {
    # generic English
    "the","and","for","with","that","this","from","have","has","will","would",
    "could","should","also","into","over","more","than","about","after","before",
    "while","during","their","there","these","those","such","other","some",
    "which","who","whom","whose","where","when","why","how",

    # discourse / academic filler
    "suggest","suggests","suggested","indicate","indicates","indicated",
    "reflect","reflects","reflected","signal","signals","signaled",
    "expect","expects","expected","likely","possibly","potential","potentially",
    "impact","impacts","effect","effects","level","levels","increase","decrease",
    "higher","lower","change","changes","changed","remain","remains",

    # finance-generic (too broad)
    "oil","market","markets","price","prices","volatility","risk","news",
    "today","report","reports","reported","according","based"
}

# ----------------------------------------
# Tokenize & filter
# ----------------------------------------
words = [
    w for w in text_all.split()
    if len(w) >= 4 and w not in stopwords
]

# ----------------------------------------
# Count frequencies
# ----------------------------------------
counts = Counter(words)

TOP_N = 15
top_terms = counts.most_common(TOP_N)

terms = [t for t,_ in top_terms]
freqs = [c for _,c in top_terms]

# ----------------------------------------
# Plot Figure 6
# ----------------------------------------
plt.figure(figsize=(9,4))
plt.barh(terms[::-1], freqs[::-1])
plt.xlabel("Frequency")
plt.ylabel("Term")
plt.title("Figure 6: Dominant Economic Narratives in LLM Rationales")
plt.tight_layout()

plt.savefig(
    "/content/results/paper_figures/Fig6_top_terms_clean_H1.png",
    dpi=200
)
plt.show()

print("Saved cleaned Figure 6 to:")
print("/content/results/paper_figures/Fig6_top_terms_clean_H1.png")



